{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/facebookresearch/pytorch_GAN_zoo\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from pytorch_GAN_zoo.models.loss_criterions import base_loss_criterions\n",
    "from pytorch_GAN_zoo.models.loss_criterions.gradient_losses import WGANGPGradientPenalty\n",
    "from pytorch_GAN_zoo.models.utils.utils import finiteCheck\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "from datetime import datetime\n",
    "from IPython import display\n",
    "\n",
    "import utils\n",
    "import pgan_model\n",
    "import batch_iterator\n",
    "\n",
    "training_timestamp = str(int(time.time()))\n",
    "model_dir = f'trained_models/model_{training_timestamp}/'\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_log(text):\n",
    "    print(text)\n",
    "    print(text, file=open(f'{model_dir}/log.txt', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy2('./pgan_train.ipynb', model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = torch.load('preprocessed_data/processed_data_4x4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "batch_iter = batch_iterator.BatchIterator(processed_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_images = batch_iter.next_batch()\n",
    "plt.imshow((batch_images[0][:3].permute(1, 2, 0)+1.0)*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sizes = {0: 4, \n",
    "             1: 8, \n",
    "             2: 16, \n",
    "             3: 32, \n",
    "             4: 64}\n",
    "\n",
    "scale = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_net = pgan_model.PGANDiscriminator()\n",
    "generator_net = pgan_model.PGANGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "discriminator_net.to(device)\n",
    "generator_net.to(device)\n",
    "\n",
    "optimizer_d = optim.Adam(filter(lambda p: p.requires_grad, discriminator_net.parameters()), betas=[0, 0.99], lr=learning_rate)\n",
    "optimizer_g = optim.Adam(filter(lambda p: p.requires_grad, generator_net.parameters()), betas=[0, 0.99], lr=learning_rate)\n",
    "\n",
    "optimizer_d.zero_grad()\n",
    "optimizer_g.zero_grad()\n",
    "\n",
    "print_and_log(generator_net)\n",
    "print_and_log(discriminator_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_criterion = base_loss_criterions.WGANGP(device)\n",
    "\n",
    "epsilon_d = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_and_log(f\"{datetime.now()} Starting Training\")\n",
    "\n",
    "n_scales = 5\n",
    "model_alpha = 0.0\n",
    "alpha_update_cons = 0.00125\n",
    "epoch_per_scale = 16\n",
    "\n",
    "show_scaled_img = False\n",
    "normalize_t = torch.Tensor([0.5]).to(device)\n",
    "scale_t = torch.Tensor([255]).to(device)\n",
    "\n",
    "fixed_latent = torch.randn(4, 512).to(device)\n",
    "generated_img_cubes = []\n",
    "\n",
    "for scale in range(0, n_scales):\n",
    "    if scale > 0:\n",
    "        model_alpha = 1.0\n",
    "        \n",
    "    if scale == 1:\n",
    "        processed_data = torch.load('preprocessed_data/processed_data_8x8.pt')\n",
    "    if scale == 2:\n",
    "        processed_data = torch.load('preprocessed_data/processed_data_16x16.pt')\n",
    "    if scale == 3:\n",
    "        processed_data = torch.load('preprocessed_data/processed_data_32x32.pt')\n",
    "    if scale == 4:\n",
    "        processed_data = torch.load('preprocessed_data/processed_data.pt')\n",
    "    \n",
    "    batch_iter = batch_iterator.BatchIterator(processed_data, batch_size)\n",
    "    print_and_log(f\"{datetime.now()} Starting scale:{scale}\")\n",
    "    \n",
    "    if show_scaled_img:\n",
    "        batch_images = batch_iter.next_batch()\n",
    "        if scale < 3:\n",
    "            plt.imshow((batch_images[0][:3].permute(1, 2, 0)+1.0)*0.5)\n",
    "            plt.show()\n",
    "        else:\n",
    "            cur_img_plane = ((batch_images[0].type(torch.FloatTensor) / torch.Tensor([255])) - torch.Tensor([0.5])) / torch.Tensor([0.5])\n",
    "            plt.imshow((cur_img_plane[:3].permute(1, 2, 0)+1.0)*0.5)\n",
    "            plt.show()\n",
    "        \n",
    "    for batch_step in range(1, (epoch_per_scale*40000//batch_size)+1):\n",
    "        if batch_step % 25 == 0 and model_alpha > 0:\n",
    "            model_alpha = model_alpha - alpha_update_cons\n",
    "            model_alpha = 0.0 if model_alpha < 1e-5 else model_alpha\n",
    "            \n",
    "        batch_images = batch_iter.next_batch()\n",
    "        if scale >= 3:\n",
    "            batch_images = ((batch_images.type(torch.FloatTensor).to(device) / scale_t) - normalize_t) / normalize_t\n",
    "        else:\n",
    "            batch_images = batch_images.to(device)\n",
    "        \n",
    "        discriminator_net.set_alpha(model_alpha)\n",
    "        generator_net.set_alpha(model_alpha)\n",
    "        \n",
    "        optimizer_d.zero_grad()\n",
    "        \n",
    "        pred_real_d = discriminator_net(batch_images, False)\n",
    "        \n",
    "        loss_d = loss_criterion.getCriterion(pred_real_d, True)\n",
    "        \n",
    "        input_latent = torch.randn(batch_size, 512).to(device)\n",
    "        \n",
    "        pred_fake_g = generator_net(input_latent).detach()\n",
    "        pred_fake_d = discriminator_net(pred_fake_g, False)\n",
    "        \n",
    "        loss_d_fake = loss_criterion.getCriterion(pred_fake_d, False)\n",
    "        \n",
    "        loss_d += loss_d_fake\n",
    "        \n",
    "        loss_d_grad = WGANGPGradientPenalty(batch_images, pred_fake_g, discriminator_net, weight=10.0, backward=True)\n",
    "        \n",
    "        loss_epsilon = (pred_real_d[:, 0] ** 2).sum() * epsilon_d\n",
    "        loss_d += loss_epsilon\n",
    "        \n",
    "        loss_d.backward(retain_graph=True)\n",
    "        finiteCheck(discriminator_net.parameters())\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        optimizer_d.zero_grad()\n",
    "        optimizer_g.zero_grad()\n",
    "        \n",
    "        input_noise = torch.randn(batch_size, 512).to(device)\n",
    "        \n",
    "        pred_fake_g = generator_net(input_noise)\n",
    "        \n",
    "        pred_fake_d, phi_g_fake = discriminator_net(pred_fake_g, True)\n",
    "        \n",
    "        loss_g_fake = loss_criterion.getCriterion(pred_fake_d, True)\n",
    "        loss_g_fake.backward(retain_graph=True)\n",
    "        \n",
    "        finiteCheck(generator_net.parameters())\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        if batch_step == 1 or batch_step % 100 == 0:\n",
    "            print_and_log(f\"{datetime.now()} [{scale}/{n_scales}][{batch_step:05d}/{epoch_per_scale*40000//batch_size}], Alpha:{model_alpha:.4f} \"\n",
    "                          f\"Loss_G:{loss_g_fake.item():.4f}\\tLoss_D:{loss_d.item():.4f}\")\n",
    "        \n",
    "        if batch_step % 5000 == 1:\n",
    "            with torch.no_grad():\n",
    "                generated_inputs = generator_net(fixed_latent).detach()\n",
    "                generated_img_cubes += [generated_inputs.cpu().numpy().transpose(0, 2, 3, 1)]\n",
    "        \n",
    "        break\n",
    "    break\n",
    "\n",
    "generated_img_cubes = np.array(generated_img_cubes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_i = 0\n",
    "frame_i = 0 # 0-35\n",
    "eval_img_cube = generated_img_cubes[-1][img_i]\n",
    "plt.imshow((eval_img_cube[:,:,frame_i*3:(frame_i+1)*3]+1.0)*0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animgan",
   "language": "python",
   "name": "animgan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
